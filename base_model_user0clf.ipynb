{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model for baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autotime\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from heapq import nlargest\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "pd.set_option(\"display.max_columns\",20)\n",
    "pd.set_option('float_format', '{:.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data\n",
      "--------------------------------------------------\n",
      "Number of users 1\n",
      "--------------------------------------------------\n",
      "   uid            datetime   loc  app_id  traffic  day\n",
      "0    0 2016-04-21 08:42:21  8194     342    0.030   21\n",
      "1    0 2016-04-21 08:44:12  8194       1    0.008   21\n",
      "2    0 2016-04-21 08:44:47  8194     857    0.027   21\n",
      "3    0 2016-04-21 08:44:48  8194     857    0.002   21\n",
      "4    0 2016-04-21 08:44:49  8194      31    0.009   21\n",
      "--------------------------------------------------\n",
      "[21 22 23 24 25 26]\n"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "usage = pd.read_csv(r\"C:\\Users\\Brian Santoso\\CS48 - Predicting User Patterns From IoT Data\\dataset\\rm_oscillated_cleaned_data.txt\", delimiter=',', \\\n",
    "    nrows=3911,\\\n",
    "    names=['uid','datetime','loc','app_id','traffic'], \\\n",
    "    dtype={'uid': int, 'datetime': str, 'loc': int, 'app_id': int, 'trafific': float})\n",
    "usage['datetime'] = usage['datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")) # Convert to datetime object\n",
    "usage['day'] = usage['datetime'].apply(lambda x: x.day)\n",
    "n_users = len(usage['uid'].unique())\n",
    "print(\"Successfully loaded data\")\n",
    "print(\"-\"*50)\n",
    "print('Number of users', n_users)\n",
    "print('-'*50)\n",
    "print(usage.head())\n",
    "print('-'*50)\n",
    "print(usage['day'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "From the DeepApp paper, they preprocess the data according to their definition. <br>\n",
    "They use 30-min interval as a session. A window is 24 hours which consists of 48 intervals. <br>\n",
    "They did not implement a dataloader. Rather they treat each window as a batch. (**Can be fine-tuned**)<br>\n",
    "<br>\n",
    "The preprocessed data should look like this: <br>\n",
    "\n",
    "<br>data: {\n",
    "    <br>user: \n",
    "        <br>'20-Apr': {\n",
    "            <br>'tim' : [list of time in the window Shape(48,1)],\n",
    "            <br>'loc' : [lsit of loc in the window Shape(48,1)],\n",
    "            <br>'app': [list of multi-hot-code vector in the window. Shape(48, 2000)]    \n",
    "        <br>},\n",
    "        <br>'21-Apr': {\n",
    "            <br>'tim' : [list of time in the window Shape(48,1)],\n",
    "            <br>'loc' : [lsit of loc in the window Shape(48,1)],\n",
    "            <br>'app': [list of multi-hot-code vector in the window. Shape(48, 2000)]\n",
    "        <br>}, ........\n",
    "    <br>},\n",
    "    <br>user2: {\n",
    "        <br>Same pattern........\n",
    "    <br>}\n",
    "<br>}\n",
    "<br>\n",
    "<br>\n",
    "**Brief summary of the helper functions**\n",
    "- generate_input returns a train set and a test set, adds more field like ptim, app_target, loc_target, uid, tim_o, loc_o, topk, etc.\n",
    "- generate_queue returns a queue so it pops a window of a user everytime and feed to the model\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "In other words, if we use a dataloader instead. For each user, trainloader has 3 batches, Vall has 1. Each batch has 48 samples. If we want to use dataloader instead, we have to make sure each iteration of it contains the information of one user?? Or order does not matter at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c704d07796fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "print(np.zeros(48,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of intervals 48\n"
     ]
    }
   ],
   "source": [
    "# Set the variable for interval span\n",
    "span = 30\n",
    "interval_span = str(span) + 'T'\n",
    "n_intervals = int(60*24/span)\n",
    "print('# of intervals', n_intervals)\n",
    "\n",
    "# Floor the datetime then convert it to h:m:s\n",
    "#  according to the interval span such that we can aggregate the requests in a session\n",
    "usage['floored_time'] = usage['datetime'].apply(lambda x: pd.Timestamp.floor(x, freq=interval_span).time())\n",
    "\n",
    "# Map the h:m:s to interval id e.g. 1, 2, 3, 4, 5\n",
    "# Generate the intervals that will be matched to the timestamp of the data\n",
    "intervals = pd.date_range('2020/1/1', freq=interval_span, periods=48)\n",
    "intervals = [i.time() for i in intervals]\n",
    "\n",
    "mapper = {}\n",
    "for i, time in enumerate(intervals):\n",
    "    mapper[time] = int(i)\n",
    "\n",
    "# Map the id to the dataframe\n",
    "usage['interval_id'] = usage['floored_time'].map(mapper)\n",
    "\n",
    "# Drop the floored_time columns\n",
    "usage = usage.drop(['floored_time', 'traffic', 'datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihot code the app_id\n",
    "app_np = usage['app_id'].to_numpy()\n",
    "\n",
    "# Create the empty array to hold the one-hot-code app\n",
    "app_multihot = np.zeros([len(usage), 2000])\n",
    "\n",
    "# Set the corresponding app index to 1\n",
    "for i, app in enumerate(app_np):\n",
    "    app_multihot[i, app-1] = 1 # INDEXING STARTS FROM ZERO\n",
    "\n",
    "# Drop the app_id before doing numpy\n",
    "usage = usage.drop('app_id', axis=1)\n",
    "\n",
    "# Convert the dataframe into numpy array\n",
    "usage_np = usage.to_numpy()\n",
    "usage_np = np.concatenate([usage_np, app_multihot], 1) # add the uid, loc, day, session_id and app tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464cfa9febc245ed9b55e2b8d7daf3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with all users!\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "users = []\n",
    "visited_session = []\n",
    "session = np.zeros([2,2002])\n",
    "\n",
    "loop = tqdm(enumerate(usage_np), total = usage_np.shape[0])\n",
    "for n, i in loop:\n",
    "\n",
    "    u, loc, day, session_id, app = i[0], i[1], i[2], int(i[3]), i[4:]\n",
    "\n",
    "    if n == 0:\n",
    "        prev_session_id = session_id\n",
    "        prev_u = u\n",
    "\n",
    "    # Create a holding array for each user, padded with number of sessions per day\n",
    "    if u not in users:\n",
    "        users.append(u)\n",
    "        data[u] = {20:np.zeros([48,2002]), 21:np.zeros([48,2002]), 22:np.zeros([48,2002]), \\\n",
    "             23:np.zeros([48,2002]), 24:np.zeros([48,2002]), 25:np.zeros([48,2002]), 26:np.zeros([48,2002])}\n",
    "\n",
    "        # For tracking the pre-processing\n",
    "        # if len(users) % 10 == 0:\n",
    "        #     print('Processed {}/{} users'.format(len(users), n_users))\n",
    "        \n",
    "        loop.set_description(f\"Processed {len(users)}/{n_users} users\")\n",
    "    \n",
    "    # If the next request is still in the same interval\n",
    "    if session_id == prev_session_id:\n",
    "        request = np.expand_dims(np.concatenate([[session_id], [loc], app]), axis=0)\n",
    "        session = np.concatenate([session,request] )\n",
    "        prev_session_id = session_id\n",
    "\n",
    "    elif session_id != prev_session_id:\n",
    "\n",
    "        # print('User {} made {} requests in session {}'.format(int(u), len(session), session_id))\n",
    "        visited_session.append(session_id)\n",
    "\n",
    "        # Create a session\n",
    "        s_loc = np.expand_dims(stats.mode(session[:, 1]).mode, axis=0)\n",
    "        s_app = np.expand_dims(np.sum(session[:, 2:], axis=0), axis=0)\n",
    "        s_sid = np.expand_dims([prev_session_id], axis=0)\n",
    "\n",
    "        s = np.concatenate([s_sid, s_loc, s_app], axis=1)\n",
    "\n",
    "        data[u][day][prev_session_id] = s # Set the interval of the day\n",
    "\n",
    "        # Reset the session initiated by the new request in the sessino\n",
    "        session = request = np.expand_dims(np.concatenate([[session_id], [loc], app]), axis=0)\n",
    "        prev_session_id = session_id\n",
    "        # break\n",
    "print('Done with all users!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1990,\n",
       " 1.0: 2,\n",
       " 2.0: 1,\n",
       " 3.0: 1,\n",
       " 4.0: 1,\n",
       " 5.0: 2,\n",
       " 8.0: 1,\n",
       " 12.0: 1,\n",
       " 16.0: 1,\n",
       " 17.0: 1,\n",
       " 8194.0: 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "unique, counts = np.unique(data[0][21][17], return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on example (1 User)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for user 0, from cleaned data, only 21,22,23,24,25,26th data is avaliable\n",
    "### 21-25 will be train set, 26 will be test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From previous documentation: First 2 columns is time(session number in a day, e.g. session 0 is from 12:00 - 12:30) and location (base_id, mode:base_id with max count)\n",
    "## All else columns are just representing 2000 different apps, 1/0 representing used or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 2)\n",
      "(240, 2000)\n"
     ]
    }
   ],
   "source": [
    "##concat dataset, 1 day has 48 rows, therefore concat 5 days for training\n",
    "user0_data_concat = np.concatenate((data[0][21], data[0][22], data[0][23], data[0][24], data[0][25]), axis = 0)\n",
    "# put it in df for ez slicing\n",
    "df_train = pd.DataFrame(data = user0_data_concat)\n",
    "#remember, first 2 rows is feature, others are labels, we are doing a multi-label, mulitioutput classification\n",
    "X_train = np.array(df_train.iloc[:,0:2])\n",
    "y_train = np.array(df_train.iloc[:,2:])\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  1992  \\\n",
       "0   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "1   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "2   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "3   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "4   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "235 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "236 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "237 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "238 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "239 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "\n",
       "     1993  1994  1995  1996  1997  1998  1999  2000  2001  \n",
       "0   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "1   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "2   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "3   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "4   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "235 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "236 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "237 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "238 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "239 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "\n",
       "[240 rows x 2002 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 2)\n",
      "(48, 2000)\n"
     ]
    }
   ],
   "source": [
    "#same for test set\n",
    "df_test = pd.DataFrame(data = data[0][26])\n",
    "X_test = np.array(df_test.iloc[:,0:2])\n",
    "y_test = np.array(df_test.iloc[:,2:])\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting: user 0 (train: first 5 days, test: 6th day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just random picked one model for testing, should have a lot of models\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier ##/\n",
    "# from sklearn.linear_model import LogisticRegression ##X\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_MLP = MultiOutputClassifier(MLPClassifier(random_state=1, max_iter=300)).fit(X_train, y_train)\n",
    "pred = clf_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99843, Zero/One Loss: 0.00157, Hamming Loss: 0.00157\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "acc_list = []\n",
    "zero_one_list = []\n",
    "hamming_list = []\n",
    "\n",
    "y_test = y_test.astype(np.int64)\n",
    "pred = pred.astype(np.int64)\n",
    "#compare and calculate matrics row by row, finally take the loss of the matrics list\n",
    "for i in range(y_test.shape[0]):\n",
    "    acc = accuracy_score(y_test[i],pred[i])\n",
    "    zero_one = zero_one_loss(y_test[i],pred[i])\n",
    "    hamming = hamming_loss(y_test[i],pred[i])\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    zero_one_list.append(zero_one)\n",
    "    hamming_list.append(hamming)\n",
    "\n",
    "print(\"Accuracy: {:.5f}, Zero/One Loss: {:.5f}, Hamming Loss: {:.5f}\".format(np.mean(acc_list), np.mean(zero_one_list), np.mean(hamming_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLR Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try adding MLR \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model\n",
    "#clf_MLR = MultiOutputClassifier(LogisticRegression(random_state=0).fit(X_train, y_train))\n",
    "#pred = clf_MLR.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try adding NB Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# define the model\n",
    "clf_NB = MultiOutputClassifier(GaussianNB()).fit(X_train, y_train)\n",
    "pred = clf_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99757, Zero/One Loss: 0.00243, Hamming Loss: 0.00243\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "zero_one_list = []\n",
    "hamming_list = []\n",
    "\n",
    "y_test = y_test.astype(np.int64)\n",
    "pred = pred.astype(np.int64)\n",
    "#compare and calculate matrics row by row, finally take the loss of the matrics list\n",
    "for i in range(y_test.shape[0]):\n",
    "    acc = accuracy_score(y_test[i],pred[i])\n",
    "    zero_one = zero_one_loss(y_test[i],pred[i])\n",
    "    hamming = hamming_loss(y_test[i],pred[i])\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    zero_one_list.append(zero_one)\n",
    "    hamming_list.append(hamming)\n",
    "\n",
    "print(\"Accuracy: {:.5f}, Zero/One Loss: {:.5f}, Hamming Loss: {:.5f}\".format(np.mean(acc_list), np.mean(zero_one_list), np.mean(hamming_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_KNN = MultiOutputClassifier(KNeighborsClassifier()).fit(X_train, y_train)\n",
    "pred = clf_KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99871, Zero/One Loss: 0.00129, Hamming Loss: 0.00129\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "zero_one_list = []\n",
    "hamming_list = []\n",
    "\n",
    "y_test = y_test.astype(np.int64)\n",
    "pred = pred.astype(np.int64)\n",
    "#compare and calculate matrics row by row, finally take the loss of the matrics list\n",
    "for i in range(y_test.shape[0]):\n",
    "    acc = accuracy_score(y_test[i],pred[i])\n",
    "    zero_one = zero_one_loss(y_test[i],pred[i])\n",
    "    hamming = hamming_loss(y_test[i],pred[i])\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    zero_one_list.append(zero_one)\n",
    "    hamming_list.append(hamming)\n",
    "\n",
    "print(\"Accuracy: {:.5f}, Zero/One Loss: {:.5f}, Hamming Loss: {:.5f}\".format(np.mean(acc_list), np.mean(zero_one_list), np.mean(hamming_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ff3c76c5aebb89467a643a5bc47ad0f7f4b7ae33ec5de3111a5b1ac554d066ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
